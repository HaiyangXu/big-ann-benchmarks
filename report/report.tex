 %\documentclass[wcp,gray]{jmlr} % test grayscale version
 %\documentclass[wcp]{jmlr}% former name JMLR W\&CP
\documentclass[pmlr,x11names,,table]{jmlr}% new name PMLR (Proceedings of Machine Learning)

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
 \usepackage{floatrow}



% \documentclass[11pt, oneside]{article}

%\usepackage{geometry}
%\geometry{letterpaper}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{hyperref,cleveref}
\usepackage{comment}

\newcommand{\templatedoc}[1]{{\color{gray}:#1}}

% add other people here 

\newcommand{\harsha}[1]{{\color{green!40!blue}[\textbf{Harsha}:#1]}}
\newcommand{\martin}[1]{{\color{blue!20!red}[\textbf{Martin}:#1]}}
\newcommand{\matthijs}[1]{{\color{blue}[\textbf{Matthijs}:#1]}}
\newcommand{\georgew}[1]{{\color{orange}[\textbf{GeorgeW}:#1]}}
\newcommand{\dmitry}[1]{{\color{blue!50!purple}[\textbf{Dmitry}:#1]}}

\iffalse 

\newcommand{\harsha}[1]{}
\newcommand{\martin}[1]{}
\newcommand{\matthijs}[1]{}

\fi


\newcommand{\eg}{\emph{e.g.}\@\xspace}
\newcommand{\etc}{\emph{etc.}\@\xspace}
\newcommand{\ie}{\emph{i.e.}\@\xspace}
\newcommand{\etal}{\emph{et al}\@\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\@\xspace}


% \newtheorem{definition}{Definition}
\newcommand{\recall}[2]{${#1}$-recall$@{#2}$} 


% suggest to give it a different name compared to proposal 
\title[Billion-scale ANNS results]{Results of the NeurIPS'21 Challenge on
  Billion-Scale Approximate Nearest Neighbor Search}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}

\newcommand{\aff}[1]{\textsuperscript{#1}}

\author{%
 \Name{Harsha Vardhan Simhadri}\aff{1} \Email{harshasi@microsoft.com}\\
 \Name{George Williams\aff{2}} \Email{gwilliams@gsitechnology.com}\\
 \Name{Martin Aum\"uller\aff{3}} \Email{maau@itu.dk}\\
 \Name{Matthijs Douze\aff{4}} \Email{matthijs@fb.com}\\
 \Name{Artem Babenko\aff{5}} \Email{artem.babenko@phystech.edu}\\
 \Name{Dmitry Baranchuk\aff{5}} \Email{dbaranchuk@yandex-team.ru}\\
 \Name{Qi Chen\aff{1}} \Email{cheqi@microsoft.com}\\
 \Name{Lucas Hosseini\aff{4}} \Email{lucas.hosseini@gmail.com}\\
 \Name{Ravishankar Krishnaswamy\aff{1}} \Email{rakri@microsoft.com}\\
 \Name{Gopal Srinivasa\aff{1}} \Email{gopalsr@microsoft.com}\\
 \Name{Suhas Jayaram Subramanya\aff{6}} \Email{suhasj@cs.cmu.edu}\\
 \Name{Jingdong Wang\aff{7}} \Email{wangjingdong@baidu.com}\\
  \rm \small \\
  \aff{1} Microsoft Research 
  \aff{2} GSI Technology
  \aff{3} IT University of Copenhagen \\
  \aff{4} Meta AI Research
  \aff{5} Yandex
  \aff{6} Carnegie Mellon University
  \aff{7} Baidu
} 
 
%%\editor{Editor's name}

\iffalse 
\title{
\vspace{-40pt}
  Billion-Scale Approximate Nearest Neighbor Search Challenge}
\author{
\hspace{-20pt}  \footnotesize{Harsha Vardhan Simhadri}   \\ \hspace{-20pt} \scriptsize{Microsoft Research India} \\ \hspace{-20pt} \scriptsize{\tt harshasi@microsoft.com} \and
\hspace{-25pt}  \footnotesize{George Williams}] \\ \hspace{-25pt} \scriptsize{GSI Technology} \\ \hspace{-25pt} \scriptsize{\tt gwilliams@gsitechnology.com} \and
\hspace{-25pt}  \footnotesize{Martin Aum\"uller} \\\hspace{-30pt} \scriptsize{IT University of Copenhagen} \\ \hspace{-25pt} \scriptsize{\tt maau@itu.dk}  \and
\hspace{-20pt}  \footnotesize{Matthijs Douze} \\ \hspace{-15pt} \scriptsize{Facebook AI Research} \\ \hspace{-20pt} \scriptsize{\tt matthijs@fb.com} \and
\hspace{-25pt}  \footnotesize{Artem Babenko} \\ \hspace{-40pt} \scriptsize{Yandex} \\ \hspace{-40pt} \scriptsize{\tt artem.babenko@phystech.edu} \and
\hspace{-20pt}  \footnotesize{Dmitry Baranchuk} \\ \hspace{-20pt} \scriptsize{Yandex} \\ \hspace{-20pt} \scriptsize{\tt dbaranchuk@yandex-team.ru} \and
\hspace{-20pt}  \footnotesize{Qi Chen} \\ \hspace{-20pt} \scriptsize{Microsoft Research Asia}\\ \hspace{-20pt} \scriptsize{\tt cheqi@microsoft.com} \and
\hspace{-20pt}  \footnotesize{Lucas Hosseini} \\ \hspace{-20pt} \scriptsize{Facebook AI Research}\\ \hspace{-20pt} \scriptsize{\tt cheqi@microsoft.com} \and
\hspace{-20pt}  \footnotesize{Ravishankar Krishnaswamy} \\ \hspace{-20pt} \scriptsize{MSR India, IIT Madras} \\ \hspace{-20pt} \scriptsize{\tt rakri@microsoft.com} \and
  \footnotesize{Gopal Srinivasa} \\\scriptsize{Microsoft Research India} \\ \scriptsize{\tt gopalsr@microsoft.com} \and
  \footnotesize{Suhas Jayaram Subramanya} \\\scriptsize{Carnegie Mellon University}\\ \scriptsize{\tt suhasj@cs.cmu.edu} \and
  \footnotesize{Jingdong Wang} \\\scriptsize{Microsoft Research Asia}\\ \scriptsize{\tt jingdw@microsoft.com} 
}
%% \date{\today}
\fi

\begin{document}

\maketitle


\begin{abstract}
  Despite the broad range of algorithms for Approximate Nearest
  Neighbor Search, most empirical evaluations of algorithms have
  focused on smaller datasets, typically of 1 million
  points~\citep{Benchmark}. However, deploying recent advances in
  embedding based techniques for search, recommendation and ranking at
  scale require ANNS indices at billion, trillion or larger
  scale. Barring a few recent papers, there is limited consensus on
  which algorithms are effective at this scale vis-\`a-vis their
  hardware cost.

  This competition\footnote{\url{https://big-ann-benchmarks.com}}
  compares ANNS algorithms by hardware cost, accuracy and
  performance. We set up an open source evaluation
  framework\footnote{\url{https://github.com/harsha-simhadri/big-ann-benchmarks/}}%~\citep{framework}
  and leaderboards for both standardized and specialized hardware.
  The stadard hardware track T1 evaluates algorithms on an Azure VM
  with limited DRAM, often the bottleneck in serving billion-scale
  indices, where the embedding data can be hundreds of GigaBytes in
  size.  It uses FAISS~\citep{Faiss17} as the baseline.  The standard
  hardware track T2 additional allows inexpensive SSDs in addition to
  the limited DRAM and uses DiskANN~\citep{DiskANN19} as the baseline.
  The specialized hardware track T3 allows any hardware configuration.
  
  We compiled six diverse billion-scale datasets, four
  newly released for this competition, that span a variety of
  modalities, data types, dimensions, deep learning models, distance
  functions and sources.  The outcome of the competition was ranked
  leaderboards of algorithms in each track based on recall at a query
  throughput threshold. Additionally, for Track T3, separate
  leaderboards were also created based on recall as well as
  cost-normalized and power-normalized query throughput.
 
\end{abstract}


\begin{keywords}
Approximate nearest neighbor search, large-scale search
\end{keywords}

\input{competition}
\input{organization}
%\input{resources}
\input{results}



\bibliographystyle{plain}
\bibliography{ref}


\end{document}
