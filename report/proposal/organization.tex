% !TEX root = proposal.tex



\section{Organizational aspects}
\subsection{Protocol}

%% Explain the procedure of the competition: what the participants will
%% have to do, what will be submitted (results or code), and the
%% evaluation procedure.  Will there be several phases? Will you use a
%% competition platform with on-line submissions and a leader board?
%% Indicate means of preventing cheating.  Provide your plan to organize
%% beta tests of your protocol and/or platform.



The competition consists of the following phases:
\begin{itemize}
\item Participants enter the competition by submitting an expression
  of interest along with the team information. Teams requesting
  compute credits will be additionally required to submit a short
  overview of the ideas to be explored, their novelty, and share any
  experimental data on the smaller slices (10M, 100M points) of the
  datasets to back the merits of the proposal. We will use this
  proposal to screen for serious attempts and limit fragmenting
  limited compute credits.
\item Participants start iterating on ideas, and submit their code in private
  (e.g. by providing read access to a private repo), with support for
  a standard interface that the evaluation platform can invoke, if
  they want their entry up on the leaderboard.
\item At the end of the competition period, participants are expected
  to release their code for transparency and open discussion, and
  submit a short report on their algorithm and the results obtained.
\item In the final phase leading up to NeurIPS'21, the organizers will
  run the code provided on a standardized set of machines, and post
  the results to the final leaderboard.
\end{itemize}

Before the start of competitions, we will release the datasets, set up
the baseline algorithms along with wrappers that automate the process
of downloading the dataset to a local drive, building an index, and
then searching the query set. Organizers will beta test this before
release and we expect that participants can copy and adapt the wrapper
code.  Based on data generated by automated scripts, we will populate
the leaderboard.


\subsection{Rules}


%% \martin{\url{https://arxiv.org/pdf/2012.07976.pdf} is a nice reference.}

\paragraph{Draft of Rules.} 

These rules apply mainly to tracks T1 and T2. 
Due to the hardware constraints of T3, there is a specific set of rules for that track.

\begin{enumerate}
  \item Participants are expected to form teams.  There are no limits
    on the number of participants in each team. % \matthijs{is it useful to distinguish individual participants from teams?} 
\item Each participant can only be in one team.
\item Each team might apply to access computing units if they lack the
  infrastructure or budget for development. Organizers will decide
  distribute compute credit to support a diverse set of participants
  from across the world. We will also offer suggestions on setting up
  Azure VMs to maximize experimentation possibilities to new teams
  starting up.
\item Each team needs to publish their submission code to a public
  repository to be included in the final evaluation.  Participants are
  also required to write a wrapper for the evaluation platform and
  contribute a set of parameters.
  \item The submitted code will be in a Docker container, that provides a
    command line utility with predefined flags to run the indexing and
    search.
  \item All teams that want to be evaluated in the final phase must
    submit their implementation with parameter choices that build an
    index on the BIGANN dataset points within a time limit of 3
    days on a specified SKU of Azure VM. Otherwise, the entry will be judged as TLE (timelimit
    exceeded). The team is required to make sure that the index can
    be build in the allotted time.
\item A maximum of 10 parameter choices can be provided for evaluation
  of search performance. The Pareto-optimal parameter choices and the
  results obtained with them will be published.
  \item The evaluation in the final phase will be carried out on
    bare-metal machines in control of the organizers. Hardware details
    have to be fixed, but we will use mid-range workstations with
    commodity hardware so that the results are indicative of the
    algorithm's performance without the need for cutting edge or
    expensive hardware.  Organizers will suggest similar cloud units
    for testing.
  %% \item In the final phase, each team is allowed to specify a single
  %%   set of parameters used for building the index, i.e., build a
  %%   single index per dataset, and provide at most 10 query
  %%   parameters.   \item A team submitting code that (explicitly) tries to cache the results of queries will be disqualified.
  \item 
    In the leaderboard, participants will be ranked based on the average QPS measure $Q_\mathrm{avg}$ (Definition~\ref{def:averagedmetric}).
      \item Each team needs to submit a report that describes details of
    the solution to be eligible for winning in Phase 3. This will be
    released to the public to share the learning from the competition.

%%  \item In the final phase, each team is allowed to specify a single set of parameters used for building the index, i.e., build a single index per dataset, and provide at most 10 query parameters. The team is required to make sure that the index can be built in the allotted time. 
  \item A team submitting code that (explicitly) tries to cache the results of queries will be disqualified.
  \item Co-organizers of this competition are allowed to participate, but are not included in the final ranking.
  \item No prize money will be awarded. 
\end{enumerate}

%% \martin{If we add prize money, there is usually a large chunk of legal terms to be added.} 


\paragraph{Discussion of Rules.}  
Our primary goal is to motivate researchers and practitioners in
industry to try out the ideas for nearest-neighbor search on such a
scale. At the moment, we know of some published baselines that are
suitable in our
setting~\cite{deep1b-link,babenko2014inverted,groh2019ggnn}, but many
existing approaches, e.g., those mentioned in~\cite{Benchmark}, could
be considered on such a scale as well. Thus, we leave the environment
as open as possible (only requiring a wrapper to the evaluation
framework).  By requiring authors to provide a write-up of their
entry, the research community and practitioners will be able to build
up on the ideas brought forward.

{\bf Inclusive rules for broader participation}. Since the step from
million- to billion-scale ANNS is quite large, and requires resources
for development and testing, we will enable teams with compute
resources where there is need. Further, the setting of tracks T1, T2
have been made as simple as possible (commodity machines with no GPUs
or accelerators) so that it is resembles machine accessible to
participants from universities, including smaller research groups
without expensive clusters.  We will apply basic filter to compute
requests based on the viability of their suggested approach and data
on smaller datasets.

%% \textbf{TBD}
%% \matthijs{something to add here? }

\paragraph{Cheating Prevention.}

We explicitly forbid teams to try to setup caches that could
precompute answers to queries. Of course, hardware features can act
like a cache and that is why we carry out the final evaluation on
private bare-metal machines that are under the organizers control.
Furthermore, we require all submissions to be open sourced so that the
code can be inspected (the open sourcing requirement is for the ANN
specific code and does not apply to drivers, firmware or generic math
libraries).  The organizers will take care of such a check.  Finally,
the final evaluation will be run on a secret query set that is not
known to the participants at time of submission.

%Choose inclusive rules, which allow the broadest possible participation from the NeurIPS audience.

\paragraph{Draft of Rules for the T3 Competition}

\begin{enumerate}
  \item Participants must indicate that they wish to compete in the T3
    competition. The coordinators will reach out to these teams in
    order to ascertain their eligibility. We can accommodate certain
    kinds custom hardware, such as non-standard GPUs, PCI-based FPGAs
    and AI accelerators.  We will make eligibility decisions on a
    case-by-case basis. The decision will be based on the level of
    effort required to support the participant and their hardware. One
    key factor to acceptance will be our ability to assess power
    consumption of the hardware.  We will be evaluating participants
    using the \emph{(queries/sec)/watt} metric acquired during the run
    of the benchmark workloads. We will also require evidence of the
    hardware cost in the form of manufactured suggested retail price
    (MSRP).

  \item We prefer that accepted participants send their custom
    hardware to the evaluation team to be installed into one of our
    bare-metal systems. This will make it easier for us to measure
    power consumption accurately and fairly across all participants.
    That said, we will make every effort to accommodate participants
    that must run their own private systems.  During on boarding, we
    will work with these T3 participants to understand how we can
    acquire the right power consumption information.

  \item Upon acceptance, participants that intend to send their
    hardware will be given special shipping instructions (at the
    participant's expense.)  The evaluation team will use the
    participant supplied installation instructions to install the
    hardware and software drivers into one of our bare-metal systems.
    Once installed, the participant will be given temporary access to
    ensure proper installation.

  \item Every team accepted into T3 will submit by publishing their
    submission code to a public repository to be included in the final
    evaluation. All teams (even teams running their own private
    systems) must publish their submission code.  This submission does
    not need to contain the participant's driver source code or any
    custom firmware code.  Each team needs to submit a report that
    describes details of the solution to be eligible for winning in
    Phase 3.

  \item The submitted code will be in a Docker container, that
    provides a command line utility with predefined flags to run the
    indexing and search. For T3 participants who have sent hardware,
    the evaluation team will ensure that the container runs on the
    bare-metal systems that have been properly configured for that
    participant's hardware. All T3 participants must submit their
    Docker container.  This includes teams that run their own private
    hardware.

  %% \item All teams that want to be evaluated in the final phase must
  %%   submit their implementation with parameter choices that finishes
  %%   on a sample dataset of XXXM points within a time limit of XXX.
  %%   Otherwise, the entry will be judged as TLE (timelimit
  %%   exceeded).\matthijs{don't forget to fill in something!}

  \item The evaluation in the final phase will be carried out either
    1) on bare-metal machines with the participant hardware installed
    2) participants running their own private hardware will run
    evaluation programs provided by the organizers. T3 participants
    that run the evaluation programs in their private systems must send
    the program's obfuscated output file to the evaluation team.

  \item In the final phase, each team is allowed to specify a single
    set of parameters used for building the index, i.e., build a
    single index per dataset, and provide at most 10 query
    parameters. The team is required to make sure that the index can
    be built in the allotted time.  T3 participants that run the
    evaluation programs in their private systems must send the script's
    obfuscated output file to the evaluation team.

  \item A team submitting code that (explicitly) tries to cache the
    results of queries will be disqualified.

  \item Co-organizers of this competition are allowed to participate,
    but are not included in the final ranking.

  \item Hardware will be sent back to participant at their expense.

\end{enumerate}

\paragraph{Discussion of T3 Rules.}
Our primary goal in T3 is to evaluate non-traditional custom hardware that supports large-scale ANN.
We do not require that participants reveal any proprietary information during the course of the competition.  For
participant's that run their own hardware, we rely largely on the "honor system," although a few cheating
prevention measures will be employed. We explicitly forbid teams to try to setup caches. 

\paragraph{Cheating Prevention in T3.} 

For participants who send their hardware, the evaluation team will carry out the final evaluation on 
bare-metal machines that are under the organizers control. Participant's will not be allowed to access 
these systems after initial installation of the hardware.  

For participants running their own private hardware, we rely heavily on the "honor system." 
That said, several cheating prevention measures will be employed:

\begin{itemize}
\item The evaluation programs sent to participants will not be plain-text scripts, but rather obfuscated binaries.
\item The evaluation programs will emit output files that will not be plain-text and will be obfuscated.
\item The evaluation program output files will contain execution time-stamps and will also capture the command line parameters used when 
the evaluation program was invoked.  Participants will send these output files to the evaluation team.
\item We will employ other measures to ensure that the participant's docker code has not deviated from what was already submitted.
\end{itemize}

%Choose inclusive rules, which allow the broadest possible participation from the NeurIPS audience.

\subsection{Schedule and readiness}

We roughly aim for the following timeline:
\begin{itemize}
 \item {\bf April 30th}: Announcement of competition, release of data
   and guidelines (if selected), and a call for participation.
 \item {\bf June 15th}: Code and testing infrastructure released.
    Participants in need of compute resources will be required to
    submit an expression of interest.
 \item {\bf June 30th}: Allocation of compute resources and start of
   the competition timeline.
  \item {\bf Oct 15th}: End of competition period and submission of
    code and report (a week apart).
  \item {\bf Nov 10th or sooner}: Review of code by organizers and
    participants. Release of preliminary results on standardized
    machines.
  \item {\bf Nov 15th or sooner}: Participants can raise concerns
    about the evaluation.
  \item {\bf Dec 5th or sooner}: Final results published, and
    competitive results archived (the competition will go on if
    interest continues).
  \item {\bf During NeurIPS}: Organizers overview the competition and
    results. Organizers also request the best entries (including
    leaderboard toppers, or promising new approaches) to present a
    10-minute overview  followed by 10 minute discussion.
\end{itemize}

At the time of proposal, BIGANN and DEEP1B datasets are ready.  The
Microsoft-SpaceV is near release on the date of this submission.  The
Microsoft-Turing-ANNS dataset is undergoing internal review and we aim
to release this in mid-April.  The data release from Yandex has
already received an approval and the dataset will be published in
mid-April.  The Facebook dataset is undergoing internal review.
% \harsha{Facebook and Yandex dataset dates to be added}
% \matthijs{Facebook dataset: the same, waiting for approval, probably later than mid-Apr}

%  \matthijs{I asked FB if we could  contribute AWS credits. We will not have the answer before Apr 1st}
Microsoft Research has committed to providing about \$50,000 worth of
Azure credit for participants. While code infrastructure needs to be
prepared, we will reuse code \cite{Benchmark} and make modifications
necessary for the competition. We think there is sufficient time to
make the modifications and beta-test them before the platform
announcement.

%% \martin{I expect that I need one-two days to come up with
%%   prototype that we can discuss.  I commit to doing this right after
%%   acceptance.}

%% Provide a time line for competition preparation and for running the
%% competition itself. Propose a reasonable schedule leaving enough time
%% for the organizers to prepare the event (a few months), enough time
%% for the participants to develop their methods (e.g. 90 days), enough
%% time for the organizers to review the entries, analyze and publish the
%% results.  For live/demonstration competitions, indicate how much
%% overall time you will need (we do not guarantee all competitions will
%% get the time they request). Also provide a detailed schedule for the
%% on-site contest at NeurIPS. This schedule should at least include
%% times for introduction talks/video presentations, demos by the
%% contestants, and an award ceremony.  Will the participants need to
%% prepare their contribution in advance (e.g. prepare a demonstration)
%% and bring ready-made software and hardware to the competition site?
%% Or, on the contrary, can will they be provided with everything they
%% need to enter the competition on the day of the competition? Do they
%% need to register in advance? What can they expect to be available to
%% them on the premises of the live competition (tables, outlets,
%% hardware, software and network connectivity)? What do they need to
%% bring (multiple connectors, extension cords, etc.)?


%% Indicate what, at the time of writing this proposal, is already ready.

\subsection{Invited Speakers}
We will invite speakers with long-standing experience in this field to
present at the venue and offer their perspective on the challenge and
research history in the area.  For example, we reached out to the
following speakers who expressed their interest in giving an invited
presentation:
\begin{itemize}
\item Alexandr Andoni, Columbia University, \url{http://www.cs.columbia.edu/~andoni/}
\item Anshumali Shrivastava, Rice University, \url{https://www.cs.rice.edu/~as143/}
\end{itemize}

If accepted, we will also reach out to other well-regarded
researchers/practitioners with different backgrounds.

\iffalse
, among others:
\begin{itemize}
\item Sanjoy Dasgupta, University of California, San Diego, \url{https://cseweb.ucsd.edu/~dasgupta/}
\item Hanan Samet, University of Maryland, \url{http://www.cs.umd.edu/~hjs/}
\item Julieta Martinez, Uber, \url{https://una-dinosauria.github.io/}
\end{itemize}
\fi

\subsection{Competition promotion}

%% Describe the plan that organizers have to promote participation in the
%% competition (e.g., mailing lists in which the call will be
%% distributed, invited talks, etc.).

Based on the participation of the ANN-Benchmarks~\cite{Benchmark}, 
we expect at least 10 participants to the competition with existing methods,
and we will reach out to research groups working on out-of-memory architectures.
Further, the organizers will reach out via email to the following communities:

\begin{itemize}
  \item Authors of research on ANNS algorithms at prominent venues
    (NeurIPS, ICML, VLDB, SODA,  etc.) over the last 5 years.
  \item All the authors of entries on ANNS-benchmarks.
  \item Academic and industry research groups working on this topic.
  \item Authors of popular blog posts that describe and compare ANNS
    algorithms.
  \item Teams building specialized hardware for ANNS that we are aware of.
\end{itemize}

We will also support entries from newer research teams across the
world through help with compute, and through early feedback at the
start of the competition.


%% Please also describe your plan  for attracting participants of groups under-represented at NeurIPS.
