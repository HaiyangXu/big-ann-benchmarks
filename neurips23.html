<!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <title>Big ANN Benchmarks</title>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.js"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <!-- Bootstrap -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <style>
            body { padding-top: 50px; }
            table, th, td {
              border: 1px solid black;
              border-collapse: collapse;
            }
            th, td {
              text-align: center;
              width: 900px;
              height: 50px;
            }
        </style>
        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
      </head>
         <body>

            <nav class="navbar navbar-inverse navbar-fixed-top">
              <div class="container">
                <div class="navbar-header">
                  <a class="navbar-brand" href="neurips23.html">Practical Vector Search Challenge</a>
                </div>
                <div id="navbar" class="collapse navbar-collapse">
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="neurips23.html#tracks">Tracks</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="neurips23.html#metrics">Metrics</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="neurips23.html#participate">Participate</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="neurips23.html#organizers">Organizers</a></li>
                  </ul>
                  <ul class="nav navbar-nav" color="red">
                    <li class="active"><a href="index.html">NeurIPS'21: Billion-scale ANN benchmarks</a></li>
                  </ul>
                </div><!--/.nav-collapse -->
              </div>
            </nav>

            <div class="container">
            <h2>Practical Vector Search Challenge: <a href="https://neurips.cc/Conferences/2023/CompetitionTrack"> NeurIPS'23 competition track</a></h2>

            <p>
            This competition is to encourage the development of indexing data structures and search algorithms
             for different practical variants of the Approximate Nearest Neighbor (ANN) or Vector search problem.
             that are increasingly relevant as vector search becomes commonplace.
            Specifically, we propose datasets and baselines the sparse, filtered, out-of-distribution and streaming variants of ANNS.
            These variants require adapted search algorithms and strategies with different tradeoffs.
            Participants are encouraged to develop and submit new algorithms that improve on the baselines for these variants.
            This competition aims at being accessible to participants by limiting the scale of the datasets to about 10 million points.
            The evaluation hardware is normalized to a specification (8 vCPUs and 16GB DRAM) accessible to everyone.
            This competition will build on the evaluation framework set up for the <a href="index.html">billion-scale ANNS challenge of NeurIPS 2021</a>.
            </p>  
            </div>

            <div id="tracks">
              <h2>Tracks: Datasets, Metrics and Baselines</h2>
              <p></p>
              
                         

              <TABLE>
                <TR>
                  <TD> Track </TD>
                  <TD> Dataset </TD>
                  <TD> Datatype </TD>
                  <TD> Dimensions </TD>
                  <TD> Distance </TD>
                  <TD> Range/k-NN </TD>
                  <TD> Base data </TD>
                  <TD> Sample data</TD>
                  <TD> Query data </TD>
                  <TD> Ground truth </TD>
                  <TD> Release terms </TD>
                </TR>

               <TR>
                <TD> Microsoft Turing-ANNS* </TD>
                <TD> float32 </TD>
                <TD> 100 </TD>
                <TD> L2 </TD>
                <TD> k-NN </TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/MSFT-TURING-ANNS/base1b.fbin"> 1B points</a></TD>
                <TD> N/A </TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/MSFT-TURING-ANNS/query100K.fbin">100K queries</a> </TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/MSFT-TURING-ANNS/query_gt100.bin">link</a> </TD>
                <TD> <a href="MSFT-Turing-ANNS-terms.txt" target="_blank">link to terms</a> </TD>
              </TR>
              <TR>
                <TD> Microsoft SPACEV* </TD>
                <TD> int8 </TD>
                <TD> 100 </TD>
                <TD> L2 </TD>
                <TD> k-NN </TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/spacev1b/spacev1b_base.i8bin">1B points</a></TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/spacev1b/spacev100m_base.i8bin">100M base points</a> </TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/spacev1b/query.i8bin">29.3K queries</a> </TD>
                <TD> <a href="https://comp21storage.blob.core.windows.net/publiccontainer/comp21/spacev1b/public_query_gt100.bin">link</a> </TD>
                <TD> <a href="https://github.com/microsoft/SPTAG/blob/master/datasets/SPACEV1B/LICENSE" target="_blank">O-UDA</a> </TD>
              </TR>
              <TR>
                <TD> Yandex Text-to-Image* </TD>
                <TD> float32 </TD>
                <TD> 200 </TD>
                <TD> inner-product </TD>
                <TD> k-NN </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/base.1B.fbin">1B points</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/query.learn.50M.fbin ">50M queries</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/query.public.100K.fbin">100K queries</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/t2i_new_groundtruth.public.100K.bin">link</a> </TD>
                <TD> <a href="https://creativecommons.org/licenses/by/4.0/deed.en">CC BY 4.0</a> </TD>
              </TR>
             </TABLE>
             * new datasets <BR>
              We recommend using <a href="https://github.com/axel-download-accelerator/axel" target="_blank">Axel</a> for downloading BIGANN, Facebook-SSN++, Yandex DEEP1B and T2I datasets.<BR>
              We recommend using <a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10" target="_blank">AzCopy</a> for downloading Microsoft datasets.
            </div>

            <div id="metrics">
            <h2>Metrics and Baselines</h2>
            
            </div>  

            <BR> 
              Baseline DiskANN indices for T2 can be downloaded using "azcopy copy 'https://comp21storage.blob.core.windows.net/publiccontainer/comp21/diskann-T2-baseline-indices' 'local_folder' --recursive". 
               Note that this would take some time as the indices are large. All indices were built using R and L parameters set to 100.
               Search for T2 used 16 threads and beamwidth 4. The Ls parameter was varied to tune recall vs QPS.<BR>
               <strong>Update:</strong> T2 baseline results have been modified after measuring via pybind11 interface on docker. There was a 30-40% QPS loss using this interface
               as compared to direct measurements of C++ code from commandline. As a result, the QPS target has now been lowered, and the recall is reported at this threshold.
            
            
            <div id="participate">
              <h2>Call for Participation and Timeline</h2>

              <p>
              <h4>Timeline (subject to change)</h4>
              <ul>
                  <li>June: Baseline results, testing infrastructure, CFP and final ranking metrics released.</li>
                  <li>July 11th: Participants in need of compute resources to submit an expression of interest.</li>
                  <li>Mid-July: Allocation of compute resources.</li>
                  <li>July 30th: Final deadline for participants to submit an expression of interest through CMT.</li>
                  <li>October 30th: End of competition period. Teams to release of code in a containerized form, and complete a pull request to the eval framework with code to run the algorithms.</li>
                  <li>Mid-November: Release of preliminary results on standardized machines. Review of code by organizers and participants. Participants can raise concerns about the evaluation.</li>
                  <li>Early December: Final results published, and competition results archived (the competition will go on if interest continues).</li>
                  <li>During NeurIPS, organizers will provide an overview of the competition and results. Organizers will also request the best entries
                    (including leaderboard toppers, or promising new approaches) to present an overview for further discussion.</li>
              </ul>
              </p>
            </div>

            <div id="organizers">
              <h2>Organizers and Dataset Contributors</h2>
              <ul>
                <li><a href="https://harsha-simhadri.org/" target="_blank">Harsha Vardhan Simhadri, Microsoft Research India</a></li>
                <li><a href="http://www.itu.dk/people/maau/" target="_blank">Martin Aumüller, IT University of Copenhagen</a> </li>
                <li><a href="https://research.yandex.com/people/610754" target="_blank">Dmitry Baranchuk, Yandex</a></li>
                <li><a href="https://ai.facebook.com/people/matthijs-douze/" target="_blank">Matthijs Douze, Facebook AI Research</a></li>
                <li><a href="" target="_blank">Amir Ingber, Pinecone</a></li>
                <li><a href="" target="_blank">Edo Liberty, Pinecone</a></li>
                <li><a href="" target="_blank">Frank Liu, Zilliz</a></li>
                <li><a href="https://medium.com/@georgewilliams" target="_blank">George Williams, GSI Technology</a> </li>
              </ul>

            <p>
              Organizers can be reached at <a href="mailto:big-ann-organizers@googlegroups.com">big-ann-organizers@googlegroups.com</a>.
            </p>
            <p>
              We thank Microsoft Research, Meta, Pinecone, Yandex and Zilliz for help in preparing and organizing this competition.
              We thank AWS, GCP, Microsoft and Pinecone for contributing compute credits. 
            </p>
            </div>
        </div>
    </body>
</html>
